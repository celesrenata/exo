#!/usr/bin/env python3
"""Simple test to validate tokenizer fix by directly testing the core logic."""

import sys
import os

# Add src to path
sys.path.insert(0, 'src')

def test_tokenizer_loading_directly():
    """Test tokenizer loading logic directly without complex setup."""
    print("üîß Testing tokenizer loading logic directly...")
    
    try:
        # Import the core components
        import torch
        from transformers import AutoTokenizer
        from exo.worker.engines.torch import TokenizerWrapper
        from exo.worker.download.download_utils import build_model_path
        print("‚úÖ Successfully imported required modules")
        
        # Test the core tokenizer loading logic that was fixed
        model_id = "microsoft/DialoGPT-medium"
        
        print(f"\nüìç Testing tokenizer loading for {model_id}...")
        
        # This is the fixed logic - tokenizer should load on ALL ranks
        for device_rank in [0, 1]:
            print(f"\n  Testing device_rank {device_rank}...")
            
            # Load tokenizer directly from model ID (this is the fixed code)
            # OLD CODE (BROKEN): if device_rank == 0: tokenizer_raw = AutoTokenizer...
            # NEW CODE (FIXED): Always load tokenizer on all ranks
            print(f"    Loading tokenizer from: {model_id}")
            tokenizer_raw = AutoTokenizer.from_pretrained(
                model_id,
                trust_remote_code=True,
            )
            
            if tokenizer_raw.pad_token is None:
                tokenizer_raw.pad_token = tokenizer_raw.eos_token
            
            tokenizer = TokenizerWrapper(tokenizer_raw)
            
            if tokenizer is None:
                print(f"    ‚ùå FAIL: Tokenizer is None on rank {device_rank}")
                return False
            else:
                print(f"    ‚úÖ PASS: Tokenizer loaded successfully on rank {device_rank}")
                print(f"       Type: {type(tokenizer)}")
                print(f"       Vocab size: {len(tokenizer_raw.get_vocab())}")
                
                # Test basic functionality
                test_text = "Hello world"
                tokens = tokenizer.encode(test_text)
                decoded = tokenizer.decode(tokens)
                print(f"       Test: '{test_text}' -> {len(tokens)} tokens -> '{decoded}'")
        
        print("\nüéâ SUCCESS: Tokenizer loads correctly on all ranks!")
        return True
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False

def verify_fix_in_code():
    """Verify the actual fix is in the code."""
    print("\nüîç Verifying fix implementation...")
    
    try:
        with open('src/exo/worker/engines/torch/utils_torch.py', 'r') as f:
            content = f.read()
        
        # Look for the specific fix patterns
        if "# Load tokenizer (needed on all ranks" in content:
            print("‚úÖ Fix comment found")
        else:
            print("‚ùå Fix comment missing")
            return False
            
        # Check that tokenizer is loaded unconditionally
        if "tokenizer_raw = AutoTokenizer.from_pretrained(" in content:
            print("‚úÖ Unconditional tokenizer loading found")
        else:
            print("‚ùå Unconditional tokenizer loading missing")
            return False
            
        # Check that the old conditional loading is gone
        lines = content.split('\n')
        for i, line in enumerate(lines):
            if "if device_rank == 0:" in line:
                # Check if this is related to tokenizer loading
                context = '\n'.join(lines[max(0, i-2):i+5])
                if "tokenizer" in context.lower():
                    print("‚ùå Old conditional tokenizer loading still present")
                    print(f"Context:\n{context}")
                    return False
        
        print("‚úÖ Old conditional tokenizer loading removed")
        print("‚úÖ Fix is properly implemented")
        return True
        
    except Exception as e:
        print(f"‚ùå Error checking code: {e}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("üîß SIMPLE TOKENIZER FIX VALIDATION")
    print("=" * 60)
    
    # First verify the fix is in the code
    if not verify_fix_in_code():
        print("\n‚ùå Fix not properly implemented in code!")
        sys.exit(1)
    
    # Then test the actual functionality
    if test_tokenizer_loading_directly():
        print("\n" + "=" * 60)
        print("üéâ SUCCESS: Tokenizer fix validated!")
        print("The tokenizer now loads correctly on all ranks.")
        print("=" * 60)
        sys.exit(0)
    else:
        print("\n" + "=" * 60)
        print("‚ùå FAILURE: Tokenizer fix validation failed!")
        print("=" * 60)
        sys.exit(1)