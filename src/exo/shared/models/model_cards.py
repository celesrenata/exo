from exo.shared.types.memory import Memory
from exo.shared.types.models import ModelId, ModelMetadata
from exo.utils.pydantic_ext import CamelCaseModel


class ModelCard(CamelCaseModel):
    short_id: str
    model_id: ModelId
    name: str
    description: str
    tags: list[str]
    metadata: ModelMetadata


MODEL_CARDS: dict[str, ModelCard] = {
    # CPU-compatible models (PyTorch/Transformers) - CausalLM models only
    "dialogpt-medium-cpu": ModelCard(
        short_id="dialogpt-medium-cpu",
        model_id=ModelId("microsoft/DialoGPT-medium"),
        name="DialoGPT Medium (CPU)",
        description="""Multi-turn dialogue response generation model for CPU inference.""",
        tags=["cpu", "small"],
        metadata=ModelMetadata(
            model_id=ModelId("microsoft/DialoGPT-medium"),
            pretty_name="DialoGPT Medium (CPU)",
            storage_size=Memory.from_mb(863),
            n_layers=24,
            hidden_size=1024,
            supports_tensor=True,
        ),
    ),
    "tinyllama-1b-cpu": ModelCard(
        short_id="tinyllama-1b-cpu",
        model_id=ModelId("TinyLlama/TinyLlama-1.1B-Chat-v1.0"),
        name="TinyLlama 1.1B Chat (CPU)",
        description="""Small chat-tuned Llama-style model for CPU inference.""",
        tags=["cpu", "small"],
        metadata=ModelMetadata(
            model_id=ModelId("TinyLlama/TinyLlama-1.1B-Chat-v1.0"),
            pretty_name="TinyLlama 1.1B Chat (CPU)",
            storage_size=Memory.from_gb(2.2),
            n_layers=22,
            hidden_size=2048,
            supports_tensor=True,
        ),
    ),
    "smollm2-360m-cpu": ModelCard(
        short_id="smollm2-360m-cpu",
        model_id=ModelId("HuggingFaceTB/SmolLM2-360M-Instruct"),
        name="SmolLM2 360M Instruct (CPU)",
        description="""Compact instruction-tuned model for CPU inference.""",
        tags=["cpu", "tiny"],
        metadata=ModelMetadata(
            model_id=ModelId("HuggingFaceTB/SmolLM2-360M-Instruct"),
            pretty_name="SmolLM2 360M Instruct (CPU)",
            storage_size=Memory.from_mb(724),
            n_layers=24,
            hidden_size=960,
            supports_tensor=True,
        ),
    ),
    
    # CUDA-compatible models - using working models, engine selection at runtime
    "dialogpt-medium-cuda": ModelCard(
        short_id="dialogpt-medium-cuda",
        model_id=ModelId("microsoft/DialoGPT-medium"),
        name="DialoGPT Medium (CUDA)",
        description="""Multi-turn dialogue response generation model for CUDA inference.""",
        tags=["cuda", "small"],
        metadata=ModelMetadata(
            model_id=ModelId("microsoft/DialoGPT-medium"),
            pretty_name="DialoGPT Medium (CUDA)",
            storage_size=Memory.from_mb(863),
            n_layers=24,
            hidden_size=1024,
            supports_tensor=True,
        ),
    ),
    "tinyllama-1b-cuda": ModelCard(
        short_id="tinyllama-1b-cuda",
        model_id=ModelId("TinyLlama/TinyLlama-1.1B-Chat-v1.0"),
        name="TinyLlama 1.1B Chat (CUDA)",
        description="""Small chat-tuned Llama-style model for CUDA inference.""",
        tags=["cuda", "small"],
        metadata=ModelMetadata(
            model_id=ModelId("TinyLlama/TinyLlama-1.1B-Chat-v1.0"),
            pretty_name="TinyLlama 1.1B Chat (CUDA)",
            storage_size=Memory.from_gb(2.2),
            n_layers=22,
            hidden_size=2048,
            supports_tensor=True,
        ),
    ),
    "smollm2-360m-cuda": ModelCard(
        short_id="smollm2-360m-cuda",
        model_id=ModelId("HuggingFaceTB/SmolLM2-360M-Instruct"),
        name="SmolLM2 360M Instruct (CUDA)",
        description="""Compact instruction-tuned model for CUDA inference.""",
        tags=["cuda", "tiny"],
        metadata=ModelMetadata(
            model_id=ModelId("HuggingFaceTB/SmolLM2-360M-Instruct"),
            pretty_name="SmolLM2 360M Instruct (CUDA)",
            storage_size=Memory.from_mb(724),
            n_layers=24,
            hidden_size=960,
            supports_tensor=True,
        ),
    ),
    
    # MLX-specific models (existing ones)
    # deepseek v3
    # "deepseek-v3-0324:4bit": ModelCard(
    #     short_id="deepseek-v3-0324:4bit",
    #     model_id="mlx-community/DeepSeek-V3-0324-4bit",
    #     name="DeepSeek V3 0324 (4-bit)",
    #     description="""DeepSeek V3 is a large language model trained on the DeepSeek V3 dataset.""",
    #     tags=[],
    #     metadata=ModelMetadata(
    #         model_id=ModelId("mlx-community/DeepSeek-V3-0324-4bit"),
    #         pretty_name="DeepSeek V3 0324 (4-bit)",
    #         storage_size=Memory.from_kb(409706307),
    #         n_layers=61,
    #     ),
    # ),
    # "deepseek-v3-0324": ModelCard(
    #     short_id="deepseek-v3-0324",
    #     model_id="mlx-community/DeepSeek-v3-0324-8bit",
    #     name="DeepSeek V3 0324 (8-bit)",
    #     description="""DeepSeek V3 is a large language model trained on the DeepSeek V3 dataset.""",
    #     tags=[],
    #     metadata=ModelMetadata(
    #         model_id=ModelId("mlx-community/DeepSeek-v3-0324-8bit"),
    #         pretty_name="DeepSeek V3 0324 (8-bit)",
    #         storage_size=Memory.from_kb(754706307),
    #         n_layers=61,
    #     ),
    # ),
    "deepseek-v3.1-4bit": ModelCard(
        short_id="deepseek-v3.1-4bit",
        model_id=ModelId("mlx-community/DeepSeek-V3.1-4bit"),
        name="DeepSeek V3.1 (4-bit)",
        description="""DeepSeek V3.1 is a large language model trained on the DeepSeek V3.1 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/DeepSeek-V3.1-4bit"),
            pretty_name="DeepSeek V3.1 (4-bit)",
            storage_size=Memory.from_gb(378),
            n_layers=61,
            hidden_size=7168,
            supports_tensor=True,
        ),
    ),
    "deepseek-v3.1-8bit": ModelCard(
        short_id="deepseek-v3.1-8bit",
        model_id=ModelId("mlx-community/DeepSeek-V3.1-8bit"),
        name="DeepSeek V3.1 (8-bit)",
        description="""DeepSeek V3.1 is a large language model trained on the DeepSeek V3.1 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/DeepSeek-V3.1-8bit"),
            pretty_name="DeepSeek V3.1 (8-bit)",
            storage_size=Memory.from_gb(713),
            n_layers=61,
            hidden_size=7168,
            supports_tensor=True,
        ),
    ),
    # "deepseek-v3.2": ModelCard(
    #     short_id="deepseek-v3.2",
    #     model_id=ModelId("mlx-community/DeepSeek-V3.2-8bit"),
    #     name="DeepSeek V3.2 (8-bit)",
    #     description="""DeepSeek V3.2 is a large language model trained on the DeepSeek V3.2 dataset.""",
    #     tags=[],
    #     metadata=ModelMetadata(
    #         model_id=ModelId("mlx-community/DeepSeek-V3.2-8bit"),
    #         pretty_name="DeepSeek V3.2 (8-bit)",
    #         storage_size=Memory.from_kb(754706307),
    #         n_layers=61,
    #         hidden_size=7168,
    #     ),
    # ),
    # "deepseek-v3.2-4bit": ModelCard(
    #     short_id="deepseek-v3.2-4bit",
    #     model_id=ModelId("mlx-community/DeepSeek-V3.2-4bit"),
    #     name="DeepSeek V3.2 (4-bit)",
    #     description="""DeepSeek V3.2 is a large language model trained on the DeepSeek V3.2 dataset.""",
    #     tags=[],
    #     metadata=ModelMetadata(
    #         model_id=ModelId("mlx-community/DeepSeek-V3.2-4bit"),
    #         pretty_name="DeepSeek V3.2 (4-bit)",
    #         storage_size=Memory.from_kb(754706307 // 2),  # TODO !!!!!
    #         n_layers=61,
    #         hidden_size=7168,
    #     ),
    # ),
    # deepseek r1
    # "deepseek-r1-0528-4bit": ModelCard(
    #     short_id="deepseek-r1-0528-4bit",
    #     model_id="mlx-community/DeepSeek-R1-0528-4bit",
    #     name="DeepSeek-R1-0528 (4-bit)",
    #     description="""DeepSeek R1 is a large language model trained on the DeepSeek R1 dataset.""",
    #     tags=[],
    #     metadata=ModelMetadata(
    #         model_id=ModelId("mlx-community/DeepSeek-R1-0528-4bit"),
    #         pretty_name="DeepSeek R1 671B (4-bit)",
    #         storage_size=Memory.from_kb(409706307),
    #         n_layers=61,
    #         hidden_size=7168,
    #     ),
    # ),
    # "deepseek-r1-0528": ModelCard(
    #     short_id="deepseek-r1-0528",
    #     model_id="mlx-community/DeepSeek-R1-0528-8bit",
    #     name="DeepSeek-R1-0528 (8-bit)",
    #     description="""DeepSeek R1 is a large language model trained on the DeepSeek R1 dataset.""",
    #     tags=[],
    #     metadata=ModelMetadata(
    #         model_id=ModelId("mlx-community/DeepSeek-R1-0528-8bit"),
    #         pretty_name="DeepSeek R1 671B (8-bit)",
    #         storage_size=Memory.from_bytes(754998771712),
    #         n_layers=61,
    # .       hidden_size=7168,
    #     ),
    # ),
    # kimi k2
    "kimi-k2-instruct-4bit": ModelCard(
        short_id="kimi-k2-instruct-4bit",
        model_id=ModelId("mlx-community/Kimi-K2-Instruct-4bit"),
        name="Kimi K2 Instruct (4-bit)",
        description="""Kimi K2 is a large language model trained on the Kimi K2 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Kimi-K2-Instruct-4bit"),
            pretty_name="Kimi K2 Instruct (4-bit)",
            storage_size=Memory.from_gb(578),
            n_layers=61,
            hidden_size=7168,
            supports_tensor=True,
        ),
    ),
    "kimi-k2-thinking": ModelCard(
        short_id="kimi-k2-thinking",
        model_id=ModelId("mlx-community/Kimi-K2-Thinking"),
        name="Kimi K2 Thinking (4-bit)",
        description="""Kimi K2 Thinking is the latest, most capable version of open-source thinking model.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Kimi-K2-Thinking"),
            pretty_name="Kimi K2 Thinking (4-bit)",
            storage_size=Memory.from_gb(658),
            n_layers=61,
            hidden_size=7168,
            supports_tensor=True,
        ),
    ),
    # llama-3.1
    "llama-3.1-8b": ModelCard(
        short_id="llama-3.1-8b",
        model_id=ModelId("mlx-community/Meta-Llama-3.1-8B-Instruct-4bit"),
        name="Llama 3.1 8B (4-bit)",
        description="""Llama 3.1 is a large language model trained on the Llama 3.1 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Meta-Llama-3.1-8B-Instruct-4bit"),
            pretty_name="Llama 3.1 8B (4-bit)",
            storage_size=Memory.from_mb(4423),
            n_layers=32,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    "llama-3.1-8b-8bit": ModelCard(
        short_id="llama-3.1-8b-8bit",
        model_id=ModelId("mlx-community/Meta-Llama-3.1-8B-Instruct-8bit"),
        name="Llama 3.1 8B (8-bit)",
        description="""Llama 3.1 is a large language model trained on the Llama 3.1 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Meta-Llama-3.1-8B-Instruct-8bit"),
            pretty_name="Llama 3.1 8B (8-bit)",
            storage_size=Memory.from_mb(8540),
            n_layers=32,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    "llama-3.1-8b-bf16": ModelCard(
        short_id="llama-3.1-8b-bf16",
        model_id=ModelId("mlx-community/Meta-Llama-3.1-8B-Instruct-bf16"),
        name="Llama 3.1 8B (BF16)",
        description="""Llama 3.1 is a large language model trained on the Llama 3.1 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Meta-Llama-3.1-8B-Instruct-bf16"),
            pretty_name="Llama 3.1 8B (BF16)",
            storage_size=Memory.from_mb(16100),
            n_layers=32,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    "llama-3.1-70b": ModelCard(
        short_id="llama-3.1-70b",
        model_id=ModelId("mlx-community/Meta-Llama-3.1-70B-Instruct-4bit"),
        name="Llama 3.1 70B (4-bit)",
        description="""Llama 3.1 is a large language model trained on the Llama 3.1 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Meta-Llama-3.1-70B-Instruct-4bit"),
            pretty_name="Llama 3.1 70B (4-bit)",
            storage_size=Memory.from_mb(38769),
            n_layers=80,
            hidden_size=8192,
            supports_tensor=True,
        ),
    ),
    # llama-3.2
    "llama-3.2-1b": ModelCard(
        short_id="llama-3.2-1b",
        model_id=ModelId("mlx-community/Llama-3.2-1B-Instruct-4bit"),
        name="Llama 3.2 1B (4-bit)",
        description="""Llama 3.2 is a large language model trained on the Llama 3.2 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Llama-3.2-1B-Instruct-4bit"),
            pretty_name="Llama 3.2 1B (4-bit)",
            storage_size=Memory.from_mb(696),
            n_layers=16,
            hidden_size=2048,
            supports_tensor=True,
        ),
    ),
    "llama-3.2-3b": ModelCard(
        short_id="llama-3.2-3b",
        model_id=ModelId("mlx-community/Llama-3.2-3B-Instruct-4bit"),
        name="Llama 3.2 3B (4-bit)",
        description="""Llama 3.2 is a large language model trained on the Llama 3.2 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Llama-3.2-3B-Instruct-4bit"),
            pretty_name="Llama 3.2 3B (4-bit)",
            storage_size=Memory.from_mb(1777),
            n_layers=28,
            hidden_size=3072,
            supports_tensor=True,
        ),
    ),
    "llama-3.2-3b-8bit": ModelCard(
        short_id="llama-3.2-3b-8bit",
        model_id=ModelId("mlx-community/Llama-3.2-3B-Instruct-8bit"),
        name="Llama 3.2 3B (8-bit)",
        description="""Llama 3.2 is a large language model trained on the Llama 3.2 dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Llama-3.2-3B-Instruct-8bit"),
            pretty_name="Llama 3.2 3B (8-bit)",
            storage_size=Memory.from_mb(3339),
            n_layers=28,
            hidden_size=3072,
            supports_tensor=True,
        ),
    ),
    # llama-3.3
    "llama-3.3-70b": ModelCard(
        short_id="llama-3.3-70b",
        model_id=ModelId("mlx-community/Llama-3.3-70B-Instruct-4bit"),
        name="Llama 3.3 70B (4-bit)",
        description="""The Meta Llama 3.3 multilingual large language model (LLM) is an instruction tuned generative model in 70B (text in/text out)""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Llama-3.3-70B-Instruct-4bit"),
            pretty_name="Llama 3.3 70B",
            storage_size=Memory.from_mb(38769),
            n_layers=80,
            hidden_size=8192,
            supports_tensor=True,
        ),
    ),
    "llama-3.3-70b-8bit": ModelCard(
        short_id="llama-3.3-70b-8bit",
        model_id=ModelId("mlx-community/Llama-3.3-70B-Instruct-8bit"),
        name="Llama 3.3 70B (8-bit)",
        description="""The Meta Llama 3.3 multilingual large language model (LLM) is an instruction tuned generative model in 70B (text in/text out)""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Llama-3.3-70B-Instruct-8bit"),
            pretty_name="Llama 3.3 70B (8-bit)",
            storage_size=Memory.from_mb(73242),
            n_layers=80,
            hidden_size=8192,
            supports_tensor=True,
        ),
    ),
    "llama-3.3-70b-fp16": ModelCard(
        short_id="llama-3.3-70b-fp16",
        model_id=ModelId("mlx-community/llama-3.3-70b-instruct-fp16"),
        name="Llama 3.3 70B (FP16)",
        description="""The Meta Llama 3.3 multilingual large language model (LLM) is an instruction tuned generative model in 70B (text in/text out)""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/llama-3.3-70b-instruct-fp16"),
            pretty_name="Llama 3.3 70B (FP16)",
            storage_size=Memory.from_mb(137695),
            n_layers=80,
            hidden_size=8192,
            supports_tensor=True,
        ),
    ),
    # qwen3
    "qwen3-0.6b": ModelCard(
        short_id="qwen3-0.6b",
        model_id=ModelId("mlx-community/Qwen3-0.6B-4bit"),
        name="Qwen3 0.6B (4-bit)",
        description="""Qwen3 0.6B is a large language model trained on the Qwen3 0.6B dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-0.6B-4bit"),
            pretty_name="Qwen3 0.6B (4-bit)",
            storage_size=Memory.from_mb(327),
            n_layers=28,
            hidden_size=1024,
            supports_tensor=False,
        ),
    ),
    "qwen3-0.6b-8bit": ModelCard(
        short_id="qwen3-0.6b-8bit",
        model_id=ModelId("mlx-community/Qwen3-0.6B-8bit"),
        name="Qwen3 0.6B (8-bit)",
        description="""Qwen3 0.6B is a large language model trained on the Qwen3 0.6B dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-0.6B-8bit"),
            pretty_name="Qwen3 0.6B (8-bit)",
            storage_size=Memory.from_mb(666),
            n_layers=28,
            hidden_size=1024,
            supports_tensor=False,
        ),
    ),
    "qwen3-30b": ModelCard(
        short_id="qwen3-30b",
        model_id=ModelId("mlx-community/Qwen3-30B-A3B-4bit"),
        name="Qwen3 30B A3B (4-bit)",
        description="""Qwen3 30B is a large language model trained on the Qwen3 30B dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-30B-A3B-4bit"),
            pretty_name="Qwen3 30B A3B (4-bit)",
            storage_size=Memory.from_mb(16797),
            n_layers=48,
            hidden_size=2048,
            supports_tensor=True,
        ),
    ),
    "qwen3-30b-8bit": ModelCard(
        short_id="qwen3-30b-8bit",
        model_id=ModelId("mlx-community/Qwen3-30B-A3B-8bit"),
        name="Qwen3 30B A3B (8-bit)",
        description="""Qwen3 30B is a large language model trained on the Qwen3 30B dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-30B-A3B-8bit"),
            pretty_name="Qwen3 30B A3B (8-bit)",
            storage_size=Memory.from_mb(31738),
            n_layers=48,
            hidden_size=2048,
            supports_tensor=True,
        ),
    ),
    "qwen3-80b-a3B-4bit": ModelCard(
        short_id="qwen3-80b-a3B-4bit",
        model_id=ModelId("mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit"),
        name="Qwen3 80B A3B (4-bit)",
        description="""Qwen3 80B""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit"),
            pretty_name="Qwen3 80B A3B (4-bit)",
            storage_size=Memory.from_mb(44800),
            n_layers=48,
            hidden_size=2048,
            supports_tensor=True,
        ),
    ),
    "qwen3-80b-a3B-8bit": ModelCard(
        short_id="qwen3-80b-a3B-8bit",
        model_id=ModelId("mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit"),
        name="Qwen3 80B A3B (8-bit)",
        description="""Qwen3 80B""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit"),
            pretty_name="Qwen3 80B A3B (8-bit)",
            storage_size=Memory.from_mb(84700),
            n_layers=48,
            hidden_size=2048,
            supports_tensor=True,
        ),
    ),
    "qwen3-80b-a3B-thinking-4bit": ModelCard(
        short_id="qwen3-80b-a3B-thinking-4bit",
        model_id=ModelId("mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit"),
        name="Qwen3 80B A3B Thinking (4-bit)",
        description="""Qwen3 80B Reasoning model""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-Next-80B-A3B-Thinking-4bit"),
            pretty_name="Qwen3 80B A3B (4-bit)",
            storage_size=Memory.from_mb(84700),
            n_layers=48,
            hidden_size=2048,
            supports_tensor=True,
        ),
    ),
    "qwen3-80b-a3B-thinking-8bit": ModelCard(
        short_id="qwen3-80b-a3B-thinking-8bit",
        model_id=ModelId("mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit"),
        name="Qwen3 80B A3B Thinking (8-bit)",
        description="""Qwen3 80B Reasoning model""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-Next-80B-A3B-Thinking-8bit"),
            pretty_name="Qwen3 80B A3B (8-bit)",
            storage_size=Memory.from_mb(84700),
            n_layers=48,
            hidden_size=2048,
            supports_tensor=True,
        ),
    ),
    "qwen3-235b-a22b-4bit": ModelCard(
        short_id="qwen3-235b-a22b-4bit",
        model_id=ModelId("mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit"),
        name="Qwen3 235B A22B (4-bit)",
        description="""Qwen3 235B (Active 22B) is a large language model trained on the Qwen3 235B dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-235B-A22B-Instruct-2507-4bit"),
            pretty_name="Qwen3 235B A22B (4-bit)",
            storage_size=Memory.from_gb(132),
            n_layers=94,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    "qwen3-235b-a22b-8bit": ModelCard(
        short_id="qwen3-235b-a22b-8bit",
        model_id=ModelId("mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit"),
        name="Qwen3 235B A22B (8-bit)",
        description="""Qwen3 235B (Active 22B) is a large language model trained on the Qwen3 235B dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-235B-A22B-Instruct-2507-8bit"),
            pretty_name="Qwen3 235B A22B (8-bit)",
            storage_size=Memory.from_gb(250),
            n_layers=94,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    "qwen3-coder-480b-a35b-4bit": ModelCard(
        short_id="qwen3-coder-480b-a35b-4bit",
        model_id=ModelId("mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit"),
        name="Qwen3 Coder 480B A35B (4-bit)",
        description="""Qwen3 Coder 480B (Active 35B) is a large language model trained on the Qwen3 Coder 480B dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-Coder-480B-A35B-Instruct-4bit"),
            pretty_name="Qwen3 Coder 480B A35B (4-bit)",
            storage_size=Memory.from_gb(270),
            n_layers=62,
            hidden_size=6144,
            supports_tensor=True,
        ),
    ),
    "qwen3-coder-480b-a35b-8bit": ModelCard(
        short_id="qwen3-coder-480b-a35b-8bit",
        model_id=ModelId("mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit"),
        name="Qwen3 Coder 480B A35B (8-bit)",
        description="""Qwen3 Coder 480B (Active 35B) is a large language model trained on the Qwen3 Coder 480B dataset.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/Qwen3-Coder-480B-A35B-Instruct-8bit"),
            pretty_name="Qwen3 Coder 480B A35B (8-bit)",
            storage_size=Memory.from_gb(540),
            n_layers=62,
            hidden_size=6144,
            supports_tensor=True,
        ),
    ),
    # gpt-oss
    "gpt-oss-120b-MXFP4-Q8": ModelCard(
        short_id="gpt-oss-120b-MXFP4-Q8",
        model_id=ModelId("mlx-community/gpt-oss-120b-MXFP4-Q8"),
        name="GPT-OSS 120B (MXFP4-Q8, MLX)",
        description="""OpenAI's GPT-OSS 120B is a 117B-parameter Mixture-of-Experts model designed for high-reasoning and general-purpose use; this variant is a 4-bit MLX conversion for Apple Silicon.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/gpt-oss-120b-MXFP4-Q8"),
            pretty_name="GPT-OSS 120B (MXFP4-Q8, MLX)",
            storage_size=Memory.from_kb(68_996_301),
            n_layers=36,
            hidden_size=2880,
            supports_tensor=True,
        ),
    ),
    "gpt-oss-20b-4bit": ModelCard(
        short_id="gpt-oss-20b-4bit",
        model_id=ModelId("mlx-community/gpt-oss-20b-MXFP4-Q4"),
        name="GPT-OSS 20B (MXFP4-Q4, MLX)",
        description="""OpenAI's GPT-OSS 20B is a medium-sized MoE model for lower-latency and local or specialized use cases; this MLX variant uses MXFP4 4-bit quantization.""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/gpt-oss-20b-MXFP4-Q4"),
            pretty_name="GPT-OSS 20B (MXFP4-Q4, MLX)",
            storage_size=Memory.from_kb(11_744_051),
            n_layers=24,
            hidden_size=2880,
            supports_tensor=True,
        ),
    ),
    # Needs to be quantized g32 or g16.
    "glm-4.5-air-8bit": ModelCard(
        short_id="glm-4.5-air-8bit",
        model_id=ModelId("mlx-community/GLM-4.5-Air-8bit"),
        name="GLM 4.5 Air 8bit",
        description="""GLM 4.5 Air 8bit""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/GLM-4.5-Air-8bit"),
            pretty_name="GLM 4.5 Air 8bit",
            storage_size=Memory.from_gb(114),
            n_layers=46,
            hidden_size=4096,
            supports_tensor=False,
        ),
    ),
    "glm-4.5-air-bf16": ModelCard(
        short_id="glm-4.5-air-bf16",
        model_id=ModelId("mlx-community/GLM-4.5-Air-bf16"),
        name="GLM 4.5 Air bf16",
        description="""GLM 4.5 Air bf16""",
        tags=[],
        metadata=ModelMetadata(
            model_id=ModelId("mlx-community/GLM-4.5-Air-bf16"),
            pretty_name="GLM 4.5 Air bf16",
            storage_size=Memory.from_gb(214),
            n_layers=46,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    
    # IPEX-compatible models (Intel GPU acceleration)
    "distilgpt2-ipex": ModelCard(
        short_id="distilgpt2-ipex",
        model_id=ModelId("distilbert/distilgpt2"),
        name="DistilGPT-2 (Intel IPEX)",
        description="""Distilled version of GPT-2 optimized for Intel GPU acceleration via IPEX. Small, fast model suitable for lightweight chat and text generation.""",
        tags=["ipex", "small", "intel"],
        metadata=ModelMetadata(
            model_id=ModelId("distilbert/distilgpt2"),
            pretty_name="DistilGPT-2 (Intel IPEX)",
            storage_size=Memory.from_mb(500),
            n_layers=6,
            hidden_size=768,
            supports_tensor=True,
        ),
    ),
    "bloomz-560m-ipex": ModelCard(
        short_id="bloomz-560m-ipex",
        model_id=ModelId("bigscience/bloomz-560m"),
        name="BLOOMZ 560M (Intel IPEX)",
        description="""Small BLOOM variant with multilingual capabilities, optimized for Intel GPU acceleration. Excellent for multilingual generation and completion tasks.""",
        tags=["ipex", "small", "intel", "multilingual"],
        metadata=ModelMetadata(
            model_id=ModelId("bigscience/bloomz-560m"),
            pretty_name="BLOOMZ 560M (Intel IPEX)",
            storage_size=Memory.from_mb(700),
            n_layers=24,
            hidden_size=1024,
            supports_tensor=True,
        ),
    ),
    "gpt2-small-ipex": ModelCard(
        short_id="gpt2-small-ipex",
        model_id=ModelId("ComCom/gpt2-small"),
        name="GPT-2 Small (Intel IPEX)",
        description="""Community GPT-2 small variant optimized for Intel GPU acceleration. Perfect for lightweight creative text generation and chat applications.""",
        tags=["ipex", "small", "intel"],
        metadata=ModelMetadata(
            model_id=ModelId("ComCom/gpt2-small"),
            pretty_name="GPT-2 Small (Intel IPEX)",
            storage_size=Memory.from_mb(600),
            n_layers=12,
            hidden_size=768,
            supports_tensor=True,
        ),
    ),
    "phi-3.5-mini-ipex": ModelCard(
        short_id="phi-3.5-mini-ipex",
        model_id=ModelId("microsoft/Phi-3.5-mini-instruct"),
        name="Phi-3.5 Mini Instruct (Intel IPEX)",
        description="""Microsoft's instruction-tuned small LLM optimized for Intel GPU acceleration. Designed for chat and instruction-following tasks with excellent performance on Intel hardware.""",
        tags=["ipex", "medium", "intel", "instruct"],
        metadata=ModelMetadata(
            model_id=ModelId("microsoft/Phi-3.5-mini-instruct"),
            pretty_name="Phi-3.5 Mini Instruct (Intel IPEX)",
            storage_size=Memory.from_gb(1.5),
            n_layers=32,
            hidden_size=3072,
            supports_tensor=True,
        ),
    ),
    "phi-mini-moe-ipex": ModelCard(
        short_id="phi-mini-moe-ipex",
        model_id=ModelId("microsoft/Phi-mini-MoE-instruct"),
        name="Phi Mini MoE Instruct (Intel IPEX)",
        description="""Microsoft's Mixture-of-Experts lightweight model optimized for Intel GPU acceleration. Efficient instruction-following with MoE architecture for better performance per parameter.""",
        tags=["ipex", "medium", "intel", "moe", "instruct"],
        metadata=ModelMetadata(
            model_id=ModelId("microsoft/Phi-mini-MoE-instruct"),
            pretty_name="Phi Mini MoE Instruct (Intel IPEX)",
            storage_size=Memory.from_gb(2),
            n_layers=32,
            hidden_size=2560,
            supports_tensor=True,
        ),
    ),
    
    # Larger IPEX-compatible models for advanced use cases
    "gpt-j-6b-ipex": ModelCard(
        short_id="gpt-j-6b-ipex",
        model_id=ModelId("EleutherAI/gpt-j-6b"),
        name="GPT-J 6B (Intel IPEX)",
        description="""EleutherAI's well-known 6B parameter autoregressive model optimized for Intel GPU acceleration. Excellent for general text generation and reasoning tasks.""",
        tags=["ipex", "large", "intel"],
        metadata=ModelMetadata(
            model_id=ModelId("EleutherAI/gpt-j-6b"),
            pretty_name="GPT-J 6B (Intel IPEX)",
            storage_size=Memory.from_gb(12),
            n_layers=28,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    "prometheus-7b-ipex": ModelCard(
        short_id="prometheus-7b-ipex",
        model_id=ModelId("prometheus-eval/prometheus-7b-v1.0"),
        name="Prometheus 7B v1.0 (Intel IPEX)",
        description="""Large 7B parameter model optimized for Intel GPU acceleration. Suitable for complex text generation and analysis tasks with distributed inference support.""",
        tags=["ipex", "large", "intel"],
        metadata=ModelMetadata(
            model_id=ModelId("prometheus-eval/prometheus-7b-v1.0"),
            pretty_name="Prometheus 7B v1.0 (Intel IPEX)",
            storage_size=Memory.from_gb(7.2),
            n_layers=32,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    "yi-9b-awq-ipex": ModelCard(
        short_id="yi-9b-awq-ipex",
        model_id=ModelId("TechxGenus/Yi-9B-AWQ"),
        name="Yi 9B AWQ (Intel IPEX)",
        description="""Yi-9B variant with AWQ quantization optimized for Intel GPU acceleration. Powerful model for reasoning, code generation, and Q&A with efficient quantization.""",
        tags=["ipex", "large", "intel", "quantized"],
        metadata=ModelMetadata(
            model_id=ModelId("TechxGenus/Yi-9B-AWQ"),
            pretty_name="Yi 9B AWQ (Intel IPEX)",
            storage_size=Memory.from_gb(10),
            n_layers=32,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    
    # Very large IPEX models for distributed inference
    "gpt-neox-20b-ipex": ModelCard(
        short_id="gpt-neox-20b-ipex",
        model_id=ModelId("EleutherAI/gpt-neox-20b"),
        name="GPT-NeoX 20B (Intel IPEX)",
        description="""EleutherAI's large 20B parameter autoregressive model optimized for Intel GPU acceleration. Requires distributed inference across multiple Intel GPUs for optimal performance.""",
        tags=["ipex", "xlarge", "intel", "distributed"],
        metadata=ModelMetadata(
            model_id=ModelId("EleutherAI/gpt-neox-20b"),
            pretty_name="GPT-NeoX 20B (Intel IPEX)",
            storage_size=Memory.from_gb(40),
            n_layers=44,
            hidden_size=6144,
            supports_tensor=True,
        ),
    ),
    
    # Large CPU models for multi-node sharding
    "llama-3.1-8b-cpu": ModelCard(
        short_id="llama-3.1-8b-cpu",
        model_id=ModelId("meta-llama/Llama-3.1-8B-Instruct"),
        name="Llama 3.1 8B Instruct (CPU)",
        description="""Large instruction-tuned model requiring multiple nodes for CPU inference.""",
        tags=["cpu", "large"],
        metadata=ModelMetadata(
            model_id=ModelId("meta-llama/Llama-3.1-8B-Instruct"),
            pretty_name="Llama 3.1 8B Instruct (CPU)",
            storage_size=Memory.from_gb(16),
            n_layers=32,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    "llama-3.1-70b-cpu": ModelCard(
        short_id="llama-3.1-70b-cpu", 
        model_id=ModelId("meta-llama/Llama-3.1-70B-Instruct"),
        name="Llama 3.1 70B Instruct (CPU)",
        description="""Very large instruction-tuned model requiring distributed inference across multiple nodes.""",
        tags=["cpu", "xlarge"],
        metadata=ModelMetadata(
            model_id=ModelId("meta-llama/Llama-3.1-70B-Instruct"),
            pretty_name="Llama 3.1 70B Instruct (CPU)",
            storage_size=Memory.from_gb(140),
            n_layers=80,
            hidden_size=8192,
            supports_tensor=True,
        ),
    ),
    "qwen2.5-32b-cpu": ModelCard(
        short_id="qwen2.5-32b-cpu",
        model_id=ModelId("Qwen/Qwen2.5-32B-Instruct"),
        name="Qwen2.5 32B Instruct (CPU)",
        description="""Large multilingual model optimized for distributed CPU inference.""",
        tags=["cpu", "large"],
        metadata=ModelMetadata(
            model_id=ModelId("Qwen/Qwen2.5-32B-Instruct"),
            pretty_name="Qwen2.5 32B Instruct (CPU)",
            storage_size=Memory.from_gb(64),
            n_layers=64,
            hidden_size=5120,
            supports_tensor=True,
        ),
    ),
    "mixtral-8x7b-cpu": ModelCard(
        short_id="mixtral-8x7b-cpu",
        model_id=ModelId("mistralai/Mixtral-8x7B-Instruct-v0.1"),
        name="Mixtral 8x7B Instruct (CPU)",
        description="""Mixture of Experts model requiring distributed inference for CPU.""",
        tags=["cpu", "xlarge", "moe"],
        metadata=ModelMetadata(
            model_id=ModelId("mistralai/Mixtral-8x7B-Instruct-v0.1"),
            pretty_name="Mixtral 8x7B Instruct (CPU)",
            storage_size=Memory.from_gb(90),
            n_layers=32,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    
    # Large CUDA models for multi-node sharding
    "llama-3.1-8b-cuda": ModelCard(
        short_id="llama-3.1-8b-cuda",
        model_id=ModelId("meta-llama/Llama-3.1-8B-Instruct"),
        name="Llama 3.1 8B Instruct (CUDA)",
        description="""Large instruction-tuned model for CUDA inference.""",
        tags=["cuda", "large"],
        metadata=ModelMetadata(
            model_id=ModelId("meta-llama/Llama-3.1-8B-Instruct"),
            pretty_name="Llama 3.1 8B Instruct (CUDA)",
            storage_size=Memory.from_gb(16),
            n_layers=32,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    "llama-3.1-70b-cuda": ModelCard(
        short_id="llama-3.1-70b-cuda",
        model_id=ModelId("meta-llama/Llama-3.1-70B-Instruct"),
        name="Llama 3.1 70B Instruct (CUDA)",
        description="""Very large instruction-tuned model for distributed CUDA inference.""",
        tags=["cuda", "xlarge"],
        metadata=ModelMetadata(
            model_id=ModelId("meta-llama/Llama-3.1-70B-Instruct"),
            pretty_name="Llama 3.1 70B Instruct (CUDA)",
            storage_size=Memory.from_gb(140),
            n_layers=80,
            hidden_size=8192,
            supports_tensor=True,
        ),
    ),
    "qwen2.5-32b-cuda": ModelCard(
        short_id="qwen2.5-32b-cuda",
        model_id=ModelId("Qwen/Qwen2.5-32B-Instruct"),
        name="Qwen2.5 32B Instruct (CUDA)",
        description="""Large multilingual model for CUDA inference.""",
        tags=["cuda", "large"],
        metadata=ModelMetadata(
            model_id=ModelId("Qwen/Qwen2.5-32B-Instruct"),
            pretty_name="Qwen2.5 32B Instruct (CUDA)",
            storage_size=Memory.from_gb(64),
            n_layers=64,
            hidden_size=5120,
            supports_tensor=True,
        ),
    ),
    "mixtral-8x7b-cuda": ModelCard(
        short_id="mixtral-8x7b-cuda",
        model_id=ModelId("mistralai/Mixtral-8x7B-Instruct-v0.1"),
        name="Mixtral 8x7B Instruct (CUDA)",
        description="""Mixture of Experts model for CUDA inference.""",
        tags=["cuda", "xlarge", "moe"],
        metadata=ModelMetadata(
            model_id=ModelId("mistralai/Mixtral-8x7B-Instruct-v0.1"),
            pretty_name="Mixtral 8x7B Instruct (CUDA)",
            storage_size=Memory.from_gb(90),
            n_layers=32,
            hidden_size=4096,
            supports_tensor=True,
        ),
    ),
    # "devstral-2-123b-instruct-2512-8bit": ModelCard(
    #     short_id="devstral-2-123b-instruct-2512-8bit",
    #     model_id=ModelId("mlx-community/Devstral-2-123B-Instruct-2512-8bit"),
    #     name="Devstral 2 123B Instruct 2512 (8-bit, MLX)",
    #     description="""Mistral AI's Devstral 2 123B Instruct (2512) is an agentic coding model.""",
    #     tags=[],
    #     metadata=ModelMetadata(
    #         model_id=ModelId("mlx-community/Devstral-2-123B-Instruct-2512-8bit"),
    #         pretty_name="Devstral 2 123B Instruct 2512 (8-bit, MLX)",
    #         storage_size=Memory.from_kb(133_000_000),
    #         n_layers=88,
    #         hidden_size=12288,
    #         supports_tensor=True,
    #     ),
    # ),
}
